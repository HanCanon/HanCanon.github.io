<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习通用技术：Pytorch快速入门 | HanCanonのBlog</title><meta name="author" content="HanCanon"><meta name="copyright" content="HanCanon"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="快速介绍使用Pytorch做深度学习训练的流程以及一些常用的模块与方法">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习通用技术：Pytorch快速入门">
<meta property="og:url" content="https://blog.hancanon.com/2025-4-21-PytorchDL.html">
<meta property="og:site_name" content="HanCanonのBlog">
<meta property="og:description" content="快速介绍使用Pytorch做深度学习训练的流程以及一些常用的模块与方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://han-canon-picture.oss-accelerate.aliyuncs.com/%E5%A3%B0%E4%B9%8B%E5%BD%A2.webp">
<meta property="article:published_time" content="2025-04-21T10:30:00.000Z">
<meta property="article:modified_time" content="2025-04-21T10:30:00.000Z">
<meta property="article:author" content="HanCanon">
<meta property="article:tag" content="python">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://han-canon-picture.oss-accelerate.aliyuncs.com/%E5%A3%B0%E4%B9%8B%E5%BD%A2.webp"><link rel="shortcut icon" href="https://han-canon-picture.oss-accelerate.aliyuncs.com/抹茶芭菲.webp"><link rel="canonical" href="https://blog.hancanon.com/2025-4-21-PytorchDL.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="vXoUBFQ43k60e5hZnfx98hISuS4V1jfxK5WtyFyXF44"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.19/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js',
      css: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习通用技术：Pytorch快速入门',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-04-21 18:30:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/%E8%99%B9%E5%A4%8F%E5%A4%B4%E5%83%8F.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><i class="fa-fw icon-SmartHome"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-SmartHome"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/archives/"><i class="fa-fw icon--shijian"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon--shijian"></use></svg><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><i class="fa-fw icon-yanshoubiaoqianguanli"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-yanshoubiaoqianguanli"></use></svg><span> 标签</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><i class="fa-fw icon-wenjianjia"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-wenjianjia"></use></svg><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/link/"><i class="fa-fw icon-link"></i><svg class="icon fas fa-tada" aria-hidden="true"><use xlink:href="#icon-link"></use></svg><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://han-canon-picture.oss-accelerate.aliyuncs.com/声之形.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="HanCanonのBlog"><span class="site-name">HanCanonのBlog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:2px" aria-hidden="true"><use xlink:href="#icon-sousuo"></use></svg></a></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><i class="fa-fw icon-SmartHome"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-SmartHome"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/archives/"><i class="fa-fw icon--shijian"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon--shijian"></use></svg><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/tags/"><i class="fa-fw icon-yanshoubiaoqianguanli"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-yanshoubiaoqianguanli"></use></svg><span> 标签</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/categories/"><i class="fa-fw icon-wenjianjia"></i><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-wenjianjia"></use></svg><span> 分类</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/link/"><i class="fa-fw icon-link"></i><svg class="icon fas fa-tada" aria-hidden="true"><use xlink:href="#icon-link"></use></svg><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习通用技术：Pytorch快速入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-21T10:30:00.000Z" title="发表于 2025-04-21 18:30:00">2025-04-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-21T10:30:00.000Z" title="更新于 2025-04-21 18:30:00">2025-04-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习通用技术：Pytorch快速入门"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h1>
<p>本文的定位如同本站的另外一篇博客<a href="https://blog.hancanon.com/2024-1-10-sklearnML.html">机器学习通用技术：sklearn快速入门</a>，在学习机器学习与深度学习的过程中，会涉及到实现这些算法的编程工具，在机器学习中是<code>sklearn</code>，在深度学习中，目前的主流工具是<code>Pytorch</code>。这些工具的学习与算法的学习相辅相成，但是工具中有一部分常用的通用性内容，故单开一文，可作为<code>Pytorch</code>的快速入门，迅速跑起来自己的第一个模型，也可以作为参考资料，以便遗忘时翻阅。</p>
<h1 id="pytorch构建模型"><a class="markdownIt-Anchor" href="#pytorch构建模型"></a> Pytorch构建模型</h1>
<p>首先，我们需要学习如何在<code>Pytorch</code>中构建一个神经网络模型，<code>Pytorch</code>中关于构建神经网络的包为<code>torch.nn</code>，<code>import</code>时一般写为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br></pre></td></tr></table></figure>
<p>相比于实现机器学习的<code>skalern</code>库，<code>Pytorch</code>构建神经网络模型的过程会更复杂一点，这主要是因为，神经网络通常是由多个不同的网络层组成的，可以灵活的调整网络层的顺序，参数以及组合，就像搭积木一样，因此，<code>torch.nn</code>中实现了大多数常用的网络层，并且可以通过特定的方式将其进行组合，下面介绍两种主要的构建模型的方法</p>
<h2 id="nnsequential"><a class="markdownIt-Anchor" href="#nnsequential"></a> nn.Sequential</h2>
<p><code>nn.Sequential()</code>是<code>torch.nn</code>中提供的一个顺序容器，其用法也非常简单，将网络层按顺序填入该容器中即可构建神经网络模型，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Flatten(),  <span class="comment"># 展平层</span></span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">10</span>)  <span class="comment"># 线性层</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这是一种非常直观的构建模型方法，适用于构建简单的神经网络。但是其缺点也在于此，由于其只能顺序堆叠，对于一些特殊的网络设计，例如<code>ResNet</code>中的残差设计，就不能使用该方法进行构建。</p>
<h2 id="model类"><a class="markdownIt-Anchor" href="#model类"></a> model类</h2>
<p>通过<strong>model类</strong>来构建神经网络模型是<code>Pytorch</code>中最常用，也是最通用的方法。顾名思义，这是一个类，这个类需要继承<code>nn.Module</code>，并且实现<code>__init__()</code> 和 <code>forward()</code> 两个方法，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LR</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Flatten()  <span class="comment"># 展平层</span></span><br><span class="line">        self.L = nn.Linear(<span class="number">784</span>, <span class="number">10</span>)  <span class="comment"># 线性层</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.<span class="built_in">input</span>(x)</span><br><span class="line">        x = self.L(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>首先，在<code>__init__()</code> 方法中，必须调用父类初始化，即<code>super().__init__()</code>，一般也会将各网络层的定义写在该方法中，不过这只是习惯，并不是必须的。而 <code>forward()</code> 表示前向传播，其接受神经网络的输入，并且返回神经网络的输出，例如上述代码中，输入<code>x</code>经过展平层与线性层，得到神经网络的输出并返回。当然了，由于这是一个类，所以还需要对这个类进行实例化才算完成模型的构建，即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = LR()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：初学者可能会对这种构建方法一头雾水，但这是不得不学习的，我能分享的经验就是，最开始只需要依葫芦画瓢，写多了也就逐渐理解了，事物都是螺旋上升的嘛。</p>
</blockquote>
<h1 id="pytorch训练模型"><a class="markdownIt-Anchor" href="#pytorch训练模型"></a> Pytorch训练模型</h1>
<p>然后，是如何使用<code>Pytorch</code>训练模型，训练一个神经网络需要准备模型，数据，损失函数以及优化算法，模型的构建我们在上一小节就完成了，而数据的读取则是另一个复杂的话题，我们会在后文进行讲解，本小节仅介绍如何使用<code>Pytorch</code>中自带的数据集模块</p>
<blockquote>
<p>注：在<code>Pytorch</code>自带的数据集模块中，集成了许多深度学习的经典数据集，对于新手学习算法来说，使用这些数据集就足够了。因此，关于读取数据集的通用方法，本文选择放在后面的小节进行讲解</p>
</blockquote>
<h2 id="torchvisiondatasets"><a class="markdownIt-Anchor" href="#torchvisiondatasets"></a> torchvision.datasets</h2>
<p><code>torchvision.datasets</code>是<code>Pytorch</code>中自带的数据集模块，其中包含了大量有关<strong>计算机视觉</strong>的经典数据集，例如<code>MNIST</code>，<code>CIFAR10</code>等，关于该模块包含了哪些数据集，可以参考<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/datasets.html">官方文档</a>或者<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.ac.cn/vision/stable/datasets.html">官方文档中文翻译版</a>，接下来我们将介绍如何使用该模块读取这些经典的数据集</p>
<blockquote>
<p>注1：从模块名称不难看出，该数据集模块其实是归属于<code>torchvision</code>库的，这是一个基于<code>Pytorch</code>开发的，有关计算机视觉的库，其提供了许多使用<code>Pytroch</code>做有关视觉的深度学习时常用的工具，例如经典的视觉数据集，用于跑模型demo，以及经典的有关计算机视觉的深度学习模型的实现，例如<code>ResNet</code>系列，一行代码就可以调用这些模型，甚至可以调用使用<code>ImageNet</code>预训练好的模型进行迁移学习，并且其源码都是用<code>python</code>写的，也是非常好的学习材料。但是，在安装<code>Pytorch</code>时，默认会顺便安装<code>torchvision</code>，这点从<code>Pytroch</code>官网提供的安装命令也能看得出来，因此，将其称为<code>Pytorch</code>自带的数据集模块也是蛮合理的吧。</p>
<p>注2：类似于<code>torchvision</code>这样的库还有不少，其都是<code>Pytorch</code>生态的组成部分，可以在<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/pytorch-domains">PyTorch官网</a>上查看，例如用于<code>NLP</code>领域的<code>torchtext</code>（可惜已经停止开发了），以及用于强化学习的<code>torchrl</code>等等。</p>
</blockquote>
<p>首先，我们需要<code>import</code>该模块，一般只<code>import</code>所需的数据集，以<code>MNIST</code>为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br></pre></td></tr></table></figure>
<p>除此之外我们还需要<code>import</code>两个模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 数据读取器</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  <span class="comment"># 数据预处理</span></span><br></pre></td></tr></table></figure>
<p>其中，<code>DataLoader</code>是<code>torch</code>自带的数据读取器，用于将数据集（严格来说是<code>Dataset</code>类）转换为一个<code>python</code>迭代器，以便于在训练过程中进行小批量读取，而<code>transforms</code>是<code>torchvision</code>中提供的用于处理图像数据的模块，其功能非常强大，不过在这里其作用仅仅是将图像数据转换为<code>torch</code>的<code>tensor</code>类型，完整的数据集读取代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据集</span></span><br><span class="line">data_MNIST_train = MNIST(<span class="string">&#x27;Data/mnist_data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">data_MNIST_test = MNIST(<span class="string">&#x27;Data/mnist_data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"><span class="comment"># 生成数据读取器</span></span><br><span class="line">MNIST_train = DataLoader(data_MNIST_train, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)  <span class="comment"># shuffle表示打乱</span></span><br><span class="line">MNIST_test = DataLoader(data_MNIST_test, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>其中，数据集函数<code>MNIST</code>的参数含义从左至右分别为，数据集存储路径，是否为训练集（反之为测试集），是否需要下载（第一次调用需要下载），对数据集所做的预处理（此处仅需要将数据类型转化为<code>tensor</code>即可）。而<code>DataLoader</code>的参数含义从左至右分别为，数据集（<code>Dataset</code>类），批量大小，是否打乱，至此，我们就得到了两个<code>python</code>迭代器。</p>
<blockquote>
<p>注：对于<code>python</code>生成器不熟悉的读者可以参考<a href="https://blog.hancanon.com/2025-3-12-Iterator_Generator.html">Python：迭代器与生成器</a>，其中介绍了<code>python</code>中的迭代器与生成器，以及如何使用<code>python</code>生成器进行小批量读取</p>
</blockquote>
<h2 id="损失函数与优化算法"><a class="markdownIt-Anchor" href="#损失函数与优化算法"></a> 损失函数与优化算法</h2>
<p><code>Pytorch</code>中，损失函数位于<code>torch.nn</code>中，而优化算法位于<code>torch.optim</code>中，两者均以类的形式存在，在使用时进行实例化即可，且损失函数与优化算法的选择也相对比较固定，一般来说，对于分类问题选择交叉熵损失函数，对于回归问题选择MSE损失函数，而优化算法一般在<code>SGD</code>与<code>Adam</code>中进行选择，此处我们选择使用交叉熵与<code>SGD</code>，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 损失函数与优化算法</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()  <span class="comment"># 交叉熵</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)  <span class="comment"># SGD</span></span><br></pre></td></tr></table></figure>
<p>上述代码中，<code>SGD</code>函数的参数<code>model.parameters()</code>代表模型的可学习参数，<code>lr</code>代表学习率。</p>
<blockquote>
<p>注：对于梯度下降不熟悉的读者可以参考<a href="https://blog.hancanon.com/2024-9-19-gradient_descent.html">深度学习基础：梯度下降</a></p>
</blockquote>
<h2 id="训练流程与gpu加速"><a class="markdownIt-Anchor" href="#训练流程与gpu加速"></a> 训练流程与GPU加速</h2>
<p>准备工作完成，现在可以正式开始模型的训练了，不过此处需要补充一个非常重要的内容，就是如何调用<code>GPU</code>进行加速，此处仅考虑<code>Windows</code>系统下使用单张<code>NVIDIA</code>的显卡进行模型训练，并且已经安装了正确的<code>Pytorch</code>及<code>CUDA</code>环境的情况，使用下面的代码可以输出可使用的显卡设备，如果没有可以使用的显卡设备，则输出为<code>CPU</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)  <span class="comment"># 调用GPU进行加速，device为GPU设备名，可以使用.to(device)将模型与数据放到GPU上</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注1：虽然在深度学习中<code>GPU</code>加速很重要，但是至少本文的内容用<code>CPU</code>还是跑得起来的，而且本文的代码也兼容了使用<code>CPU</code>运行的情况，不过真的要学习深度学习的话还是建议搞张N卡来跑模型，或者用<code>Apple</code>的M系列芯片来跑也是不错的选择（不过M系列芯片得改一下代码</p>
<p>注2：本文并不是一篇关于环境配置的文章，配深度学习的环境还是有点小复杂的，如果环境出了什么问题，没办法顺利调用<code>GPU</code>的话，请出门左转，配置好环境再继续阅读，或者就将就着用<code>CPU</code>先</p>
</blockquote>
<p>然后可以使用<code>to()</code>方法，将模型或者数据放到指定的设备上，这里先将模型放到<code>GPU</code>上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)  <span class="comment"># 将模型放到device上</span></span><br></pre></td></tr></table></figure>
<p>现在可以正式进入模型训练了，下面我们给出模型训练的核心代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model.train()  <span class="comment"># 调整为训练模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> MNIST_train:</span><br><span class="line">    data, target = data.to(device), target.to(device)  <span class="comment"># 将数据放到GPU上</span></span><br><span class="line">        </span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        </span><br><span class="line">    output = model(data)  <span class="comment"># 前向传播</span></span><br><span class="line">    loss = criterion(output, target)  <span class="comment"># 计算损失值</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()  <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure>
<p>首先，我们使用<code>train()</code>方法令模型进入训练模式，该模式的主要作用是让如<code>dropout</code>一类，在训练过程中开启，在推理过程中关闭的网络层可以正确的工作，在此处其实并无作用，不过为了代码的通用性，还是加上。然后是使用<code>for</code>循环配合生成器，对数据进行小批量读取，并且将读取的数据放到<code>GPU</code>上。接着对梯度进行清零，因为<code>Pytorch</code>中，优化器的梯度是会累计的，因此在每一轮训练之前都需要手动清零。再之后便是进行前向传播得到模型输出，然后计算损失值，进行反向传播，最后使用优化算法更新参数，每一步对应一行代码，大多数情况下这几行代码都是不会变的。</p>
<blockquote>
<p>注：模型和数据必须放在同一个训练设备上，否则会报错</p>
</blockquote>
<p>最后，将上述代码封装为函数，并简单加上一些信息打印，就实现了一个可以进行一个<code>epoch</code>训练的函数，代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    model.train()  <span class="comment"># 调整为训练模式</span></span><br><span class="line">    run_loss = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> MNIST_train:</span><br><span class="line">        data, target = data.to(device), target.to(device)  <span class="comment"># 将数据放到GPU上</span></span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        </span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = criterion(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        run_loss += loss.item()  <span class="comment"># 存储本mini_batch的损失值</span></span><br><span class="line">        </span><br><span class="line">    train_loss = run_loss / <span class="built_in">len</span>(MNIST_train)  <span class="comment"># 将损失值之和除以样本量，用于近似本epoch的损失值</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss))</span><br></pre></td></tr></table></figure>
<p>调用上述函数进行10个<code>epoch</code>的训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    train(epoch)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 	Training Loss: 0.479043</span><br><span class="line">Epoch: 1 	Training Loss: 0.337416</span><br><span class="line">Epoch: 2 	Training Loss: 0.314644</span><br><span class="line">Epoch: 3 	Training Loss: 0.302373</span><br><span class="line">Epoch: 4 	Training Loss: 0.294522</span><br><span class="line">Epoch: 5 	Training Loss: 0.288994</span><br><span class="line">Epoch: 6 	Training Loss: 0.284427</span><br><span class="line">Epoch: 7 	Training Loss: 0.281150</span><br><span class="line">Epoch: 8 	Training Loss: 0.278040</span><br><span class="line">Epoch: 9 	Training Loss: 0.275650</span><br></pre></td></tr></table></figure>
<p>至此，我们就使用<code>Pytorch</code>完成了第一个神经网络模型的训练。</p>
<h2 id="模型测试"><a class="markdownIt-Anchor" href="#模型测试"></a> 模型测试</h2>
<p>接着就是模型的测试了，这个就很简单了，只需要计算模型输出，再计算准确率即可，不过分类模型的输出是概率向量，因此需要将其转换为标签，另外，模型测试时需要使用<code>eval()</code>方法将模型调整为测试模型，该模式与上面的训练模式对应，最后，跑测试的时候可以关闭梯度计算，提高计算速度，具体代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():       </span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 调整为测试模式</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不需要计算梯度</span></span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> MNIST_test:</span><br><span class="line">            data, target = data.to(device), target.to(device)  <span class="comment"># 将数据放到GPU上</span></span><br><span class="line">            </span><br><span class="line">            output = model(data)  <span class="comment"># 计算输出</span></span><br><span class="line">            </span><br><span class="line">            predicted = torch.argmax(output, <span class="number">1</span>)  <span class="comment"># 向量转换为标签</span></span><br><span class="line">            </span><br><span class="line">            total += target.size(<span class="number">0</span>)  <span class="comment"># 测试集数量</span></span><br><span class="line">            correct += (predicted == target).<span class="built_in">sum</span>().item()  <span class="comment"># 预测值与正确类别是否相等，相等则+1（labels是类别数）</span></span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct / total))  <span class="comment"># 正确率</span></span><br></pre></td></tr></table></figure>
<p>调用上述函数进行模型测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test()  <span class="comment"># 输出为: Accuracy on test set: 92 %</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：经过上面的学习，不难发现，<code>Pytorch</code>训练模型是一个相对固定的流程，因此大多数时候大家都会根据自己的使用习惯写一个训练函数，包含训练，测试，结果打印，画图等功能，然后就一直用下去</p>
</blockquote>
<h2 id="模型保存与加载"><a class="markdownIt-Anchor" href="#模型保存与加载"></a> 模型保存与加载</h2>
<p>最后，我们来介绍模型的保存，<code>Pytorch</code>的模型保存非常的方便，使用<code>torch.save</code>函数即可实现，常用的有两种保存方式，一种是保存整个模型，另一种是只保存模型的参数，我们一个一个来看。首先是保存整个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存整个模型</span></span><br><span class="line">torch.save(model, <span class="string">&#x27;model.pt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>使用<code>torch.save</code>函数直接保存整个模型，保存的模型文件后缀通常为<code>.pt</code>或者<code>.pth</code>。而模型的读取也非常简单，使用<code>torch.load</code>函数即可直接读取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载整个模型</span></span><br><span class="line">loaded_model = torch.load(<span class="string">&#x27;model.pt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">loaded_model.<span class="built_in">eval</span>()  <span class="comment"># 调整为测试模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> MNIST_test:</span><br><span class="line">        data, target = data.to(device), target.to(device)  <span class="comment"># 将数据放到GPU上</span></span><br><span class="line">            </span><br><span class="line">        output = model(data)  <span class="comment"># 计算输出</span></span><br><span class="line">            </span><br><span class="line">        predicted = torch.argmax(output, <span class="number">1</span>)  <span class="comment"># 向量转换为标签</span></span><br><span class="line">            </span><br><span class="line">        total += target.size(<span class="number">0</span>)  <span class="comment"># 测试集数量</span></span><br><span class="line">        correct += (predicted == target).<span class="built_in">sum</span>().item()  <span class="comment"># 预测值与正确类别是否相等，相等则+1（labels是类别数）</span></span><br><span class="line">            </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct / total))  <span class="comment"># 正确率</span></span><br></pre></td></tr></table></figure>
<p>运行一下测试代码也没什么问题。</p>
<p>然后是只保存模型的参数，这也是<strong>最常用</strong>的模型保存方法，只需要保存的时候传入模型的参数<code>model.state_dict()</code>即可，保存的模型文件后缀不变</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅保存模型参数</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_dict.pt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>而加载模型时，由于仅保存了模型参数，因此需要先定义正确的模型结构，再加载模型参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loaded_model_dict = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))  <span class="comment"># 定义模型结构</span></span><br><span class="line">loaded_model_dict.load_state_dict(torch.load(<span class="string">&#x27;model_dict.pt&#x27;</span>))  <span class="comment"># 读取模型参数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">loaded_model_dict.<span class="built_in">eval</span>()  <span class="comment"># 调整为测试模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> data, target <span class="keyword">in</span> MNIST_test:</span><br><span class="line">        data, target = data.to(device), target.to(device)  <span class="comment"># 将数据放到GPU上</span></span><br><span class="line">            </span><br><span class="line">        output = model(data)  <span class="comment"># 计算输出</span></span><br><span class="line">            </span><br><span class="line">        predicted = torch.argmax(output, <span class="number">1</span>)  <span class="comment"># 向量转换为标签</span></span><br><span class="line">            </span><br><span class="line">        total += target.size(<span class="number">0</span>)  <span class="comment"># 测试集数量</span></span><br><span class="line">        correct += (predicted == target).<span class="built_in">sum</span>().item()  <span class="comment"># 预测值与正确类别是否相等，相等则+1（labels是类别数）</span></span><br><span class="line">            </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set: %d %%&#x27;</span> % (<span class="number">100</span> * correct / total))  <span class="comment"># 正确率</span></span><br></pre></td></tr></table></figure>
<h1 id="pytorch读取数据集"><a class="markdownIt-Anchor" href="#pytorch读取数据集"></a> Pytorch读取数据集</h1>
<p>最后，我们来解决上文的遗留问题，如何读取自己的数据集，需要注意的是，由于深度学习涉及多种类型的数据，可能是图像，文字，音频，视频等，因此其读取的方式也不尽相同，下面只会介绍两种最通用的，适用于所有数据类型的数据集读取方式，本小节我们会以一个非常经典的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition/overview">猫狗数据集</a>为例，这是一个包含25000张猫狗图像的二分类数据集，我们会以读取其训练集为目标。</p>
<h2 id="dataset类"><a class="markdownIt-Anchor" href="#dataset类"></a> Dataset类</h2>
<p>首先，我们介绍<code>Pytorch</code>官方推荐的方法，即通过自定义类，配合<code>DataLoader</code>实现数据集的读取，<code>DataLoader</code>上文我们就讲过，其作用非常简单，输入一个<code>Dataset</code>类，返回一个<code>python</code>迭代器，用于实现数据集的小批量读取，而读取数据集的关键就在于<code>Dataset</code>类的实现上。</p>
<p><code>Dataset</code>位于<code>torch.utils.data</code>，类似于<code>nn.Module</code>，<code>Dataset</code>通常作为父类被继承，并且在此基础上实现<code>__init__()</code>方法，<code>__len__()</code>方法与<code>__getitem__()</code>方法，即可被称为一个<code>Dataset</code>类，其中，<code>__init__()</code>方法用于初始化类，<code>__len__()</code>方法用于返回数据集的大小，而<code>__getitem__()</code>方法则根据索引，返回对应的样本与标签。</p>
<p>从上述对<code>Dataset</code>类的描述中不难看出，对于不同类型的数据，不同的存储方式，不同的目录结构，<code>Dataset</code>类会有不同的实现方式，只需要满足上述三个方法的要求即可，因此，这一部分的内容并不像上一小节的代码那么固定。</p>
<p>现在，我们就要定义一个<code>Dataset</code>类来读取猫狗数据集，该数据集下载下来是一个<code>.zip</code>压缩包，解压后是一个<code>train</code>文件夹，图像数据均在此文件夹中，数据均为<code>.jpg</code>格式，命名为类型加编号，例如<code>cat.0.jpg</code>，将该压缩包放置在<code>./Data/CatDog_data</code>，下面我们将采用不对数据进行提前解压，直接从压缩包中读取数据的方式，完整<code>Dataset</code>类定义如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile  <span class="comment"># 读取zip压缩包数据</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  <span class="comment"># 读取图像数据</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取猫狗数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CatDogDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, transform=<span class="literal">None</span>, label_cat=<span class="number">0</span></span>):</span><br><span class="line">        self.root = root  <span class="comment"># 数据集路径</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.label_cat = label_cat</span><br><span class="line">        <span class="keyword">if</span> self.label_cat == <span class="number">0</span>:</span><br><span class="line">            self.label_dog = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.label_dog = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> zipfile.ZipFile(self.root, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> zip_ref:</span><br><span class="line">            self.file_list = [f <span class="keyword">for</span> f <span class="keyword">in</span> zip_ref.namelist() <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.jpg&#x27;</span>)]  <span class="comment"># jpg文件列表</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">with</span> zipfile.ZipFile(self.root, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> zip_ref:</span><br><span class="line">            <span class="keyword">with</span> zip_ref.<span class="built_in">open</span>(self.file_list[idx]) <span class="keyword">as</span> file:</span><br><span class="line">                <span class="comment"># 由于PIL读取图像数据是延迟加载的，此处用PIL直接读取会导致文件关闭了，但是图像数据还没有读取，导致报错</span></span><br><span class="line">                img_data = file.read()  <span class="comment"># 将文件完全读取至内存中</span></span><br><span class="line">                </span><br><span class="line">        <span class="comment"># 使用io.BytesIO将二进制文件转化为数据流，保证不依赖zip文件</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(io.BytesIO(img_data)).convert(<span class="string">&#x27;RGB&#x27;</span>)  <span class="comment"># 保证读取图像为RGB三通道</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            img = self.transform(img)  <span class="comment"># 对图像数据做变换</span></span><br><span class="line">            </span><br><span class="line">        label_name = os.path.basename(self.file_list[idx])  <span class="comment"># 提取文件名</span></span><br><span class="line">        <span class="keyword">if</span> label_name.startswith(<span class="string">&#x27;cat&#x27;</span>):</span><br><span class="line">            label = self.label_cat</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label = self.label_dog</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):  <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br></pre></td></tr></table></figure>
<p>现在我们来讲解上述代码，首先是导入的库，<code>zipfile</code>库用于打开<code>zip</code>文件，<code>PIL</code>用于读取图像数据，其余为辅助库。然后是<code>__init__()</code>方法，即类的初始化部分，该方法接受的参数有三个，<code>root</code>为数据集压缩包的路径，<code>transform</code>为需要对数据做的处理，因为<code>torchvision</code>中的<code>transforms</code>模块集成了许多用于处理图像的函数，且不同的图像预处理会影响训练结果，因此这部分通常会在类外进行设置，通过参数传入，<code>label_cat</code>为猫的类别编码，默认为<code>0</code>。在该方法中，除了初始化<code>self</code>变量外，我们还需要读取<code>zip</code>文件中的所有图像数据的列表，使用<code>zipfile</code>打开<code>zip</code>文件并使用<code>.namelist()</code>方法返回文件列表，需要注意的是该方法会返回文件夹路径，例如<code>./train/</code>，该路径我们是不需要的，我们只需要<code>.jpg</code>文件的路径，使用<code>.endswith()</code>方法检测路径是否以<code>.jpg</code>结尾进行筛选，得到文件路径列表<code>self.file_list</code>。</p>
<p>接着，我们先来看<code>__len__()</code>方法，该方法的任务是返回数据集大小，对于该数据集就是图像的数量，我们刚才已经拿到了文件列表，文件列表的长度就是图像的数量，因此直接<code>return len(self.file_list)</code>即可。</p>
<p>最后是<code>__getitem__()</code>方法，该方法的任务是接受一个索引，返回该索引对应的数据与标签即可，而文件列表刚好就可以实现索引与图像数据的对应，因此，实现思路为根据输入索引读取文件列表中对应的图像，并根据图像名称给出类别标签。</p>
<blockquote>
<p>注：使用<code>file.read()</code>读取图像文件，而不是直接使用<code>PIL</code>读取，是因为<code>PIL</code>读取图像数据是后加载的，如果直接在<code>with</code>中使用<code>PIL</code>读取数据，会导致<code>PIL</code>在读取文件时，<code>zip</code>文件已经关闭，无法顺利读取而报错。</p>
</blockquote>
<p>最后，我们将我们定义好的<code>Dataset</code>类实例化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">catdog_path = <span class="string">&#x27;.\Data\CatDog_data\\train.zip&#x27;</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 数据集中图片大小不统一，需要统一转化为相同大小</span></span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">data_catdog = CatDogDataset(catdog_path, transform=transform_train)</span><br></pre></td></tr></table></figure>
<p>需要注意的是，在<code>transforms</code>中，相比于读取<code>MNIST</code>数据集，多了一步将所有的图像调整至统一大小。之后，再配合<code>DataLoader</code>实现小批量读取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catdog_train = DataLoader(data_catdog, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> catdog_train:</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line">    <span class="built_in">print</span>(target.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([64, 3, 224, 224])</span><br><span class="line">torch.Size([64])</span><br></pre></td></tr></table></figure>
<p>总结一下，使用自定义<code>Dataset</code>类读取数据是一种通用的数据读取方法，只需要按照定义实现指定的方法，就可以实现数据的读取，并且可以配合<code>DataLoader</code>实现数据的小批量读取，不过<code>Dataset</code>类的实现需要具体情况具体实现，并且其自由度并非最高，而且对于特定的数据，例如读取特定目录结构的图像数据，可以使用<code>torchvision</code>中的<code>ImageFolder</code>函数，其便捷程度是要更高的。</p>
<h2 id="自定义迭代器"><a class="markdownIt-Anchor" href="#自定义迭代器"></a> 自定义迭代器</h2>
<p>然后，我们来介绍一种自由度最高的数据读取方法，那就是完全手动读取，<code>Pytorch</code>训练时需要一个可以小批量读取数据的<code>python</code>迭代器，那么我们只要以写一个<code>python</code>迭代器为最终目标，其余的部分我们都可以自行定义与实现。其主要用于<code>Dataset</code>类配合<code>DataLoader</code>也无法实现全部需求的情况，例如文本数据，为了保证数据大小一致，通常会对文本数据进行填充，那么在训练时，除了需要数据本身，可能还需要文本数据的真实长度，此时由于<code>DataLoader</code>方法无法进行自定义，因此一般会选择手动读取数据。</p>
<p>现在，我们演示一下如何通过实现一个类来实现读取猫狗数据集，完整代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile  <span class="comment"># 读取zip压缩包数据</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  <span class="comment"># 读取图像数据</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CatDogData</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, batch_size, transform=<span class="literal">None</span>, label_cat=<span class="number">0</span></span>):</span><br><span class="line">        self.root = root</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.label_cat = label_cat</span><br><span class="line">        <span class="keyword">if</span> self.label_cat == <span class="number">0</span>:</span><br><span class="line">            self.label_dog = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.label_dog = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> zipfile.ZipFile(self.root, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> zip_ref:</span><br><span class="line">            self.file_list = [f <span class="keyword">for</span> f <span class="keyword">in</span> zip_ref.namelist() <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.jpg&#x27;</span>)]  <span class="comment"># jpg文件列表</span></span><br><span class="line">            </span><br><span class="line">        self.file_list = np.array(self.file_list)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.__data_iter()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__data_iter</span>(<span class="params">self</span>):</span><br><span class="line">        data_size = self.__len__()  <span class="comment"># 数据集数量</span></span><br><span class="line">        index = np.random.permutation(data_size)  <span class="comment"># 生成随机索引</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, data_size, self.batch_size):</span><br><span class="line">            batch_index = index[i: <span class="built_in">min</span>(i + self.batch_size, data_size)]  <span class="comment"># batch的索引列表</span></span><br><span class="line">            batch_file_list = self.file_list[batch_index]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">with</span> zipfile.ZipFile(self.root, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> zip_ref:</span><br><span class="line">                img_data_list = []</span><br><span class="line">                <span class="keyword">for</span> file_name <span class="keyword">in</span> batch_file_list:</span><br><span class="line">                    <span class="keyword">with</span> zip_ref.<span class="built_in">open</span>(file_name) <span class="keyword">as</span> file:</span><br><span class="line">                        img_data = file.read()  <span class="comment"># 将文件完全读取至内存中</span></span><br><span class="line">                        img_data_list.append(img_data)</span><br><span class="line">                    </span><br><span class="line">            X = []</span><br><span class="line">            y = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(np.size(batch_file_list)):</span><br><span class="line">                img = Image.<span class="built_in">open</span>(io.BytesIO(img_data_list[i])).convert(<span class="string">&#x27;RGB&#x27;</span>)  <span class="comment"># 保证读取图像为RGB三通道</span></span><br><span class="line">                <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    img = self.transform(img)</span><br><span class="line">                </span><br><span class="line">                label_name = os.path.basename(batch_file_list[i])  <span class="comment"># 提取文件名</span></span><br><span class="line">                <span class="keyword">if</span> label_name.startswith(<span class="string">&#x27;cat&#x27;</span>):</span><br><span class="line">                    label = self.label_cat</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    label = self.label_dog</span><br><span class="line">                    </span><br><span class="line">                X.append(img)</span><br><span class="line">                y.append(label)</span><br><span class="line">                </span><br><span class="line">            X = torch.stack(X, dim=<span class="number">0</span>)</span><br><span class="line">            y = torch.tensor(y)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">yield</span> X, y</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):  <span class="comment"># 返回数据集大小</span></span><br><span class="line">        <span class="keyword">return</span> np.size(self.file_list)</span><br></pre></td></tr></table></figure>
<p>首先是<code>__init__()</code>方法，除了加入了<code>batch_size</code>参数以及将<code>file_list</code>转换为<code>array</code>之外，与使用<code>Dataset</code>类的实现没有区别。然后是该类的实现重点，<code>__iter__()</code>方法与<code>__data_iter</code>函数，这里采用了与传统的<code>python</code>迭代器不同的实现方法，没有实现<code>__next__()</code>方法，而是使用<code>yield</code>关键字实现<code>python</code>迭代器，再使用<code>__iter__()</code>方法返回该迭代器，之所以必须实现<code>__iter__()</code>方法，是因为如果没有实现该方法，则该类实例化之后无法被识别为一个可迭代对象，也因此无法配合<code>for</code>循环实现小批量读取。而<code>__data_iter</code>函数中，将图像数据的读取与小批量读取同时进行实现，其中图像数据的读取代码跟<code>Dataset</code>类的实现是一致的，而实现小批量读取的代码可以参考本站的另外一篇博客<a href="https://blog.hancanon.com/2025-3-12-Iterator_Generator.html">Python：迭代器与生成器 </a>。</p>
<p>最后，将定义好的类实例化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">catdog_path = <span class="string">&#x27;E:\Code\DL\Data\CatDog_data\\train.zip&#x27;</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 数据集中图片大小不统一，需要统一转化为相同大小</span></span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line">data_catdog = CatDogData(catdog_path, batch_size=<span class="number">64</span>, transform=transform_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> data_catdog:</span><br><span class="line">    <span class="built_in">print</span>(data.shape)</span><br><span class="line">    <span class="built_in">print</span>(target.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([64, 3, 224, 224])</span><br><span class="line">torch.Size([64])</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.hancanon.com">HanCanon</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.hancanon.com/2025-4-21-PytorchDL.html">https://blog.hancanon.com/2025-4-21-PytorchDL.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.hancanon.com" target="_blank">HanCanonのBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://han-canon-picture.oss-accelerate.aliyuncs.com/声之形.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://lib.baomitu.com/social-share.js/1.0.16/css/share.min.css" media="print" onload="this.media='all'"><script src="https://lib.baomitu.com/social-share.js/1.0.16/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025-4-11-MatrixAlgebra.html" title="机器学习数学基础：矩阵微分"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景2.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习数学基础：矩阵微分</div></div></a></div><div class="next-post pull-right"><a href="/2025-9-2-firefoxconfig.html" title="Firefox自定义技巧分享"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景1.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Firefox自定义技巧分享</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025-1-30-MLP.html" title="感知机及其代码实现"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-30</div><div class="title">感知机及其代码实现</div></div></a></div><div><a href="/2024-1-10-sklearnML.html" title="机器学习通用技术：sklearn快速入门"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/声之形.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-10</div><div class="title">机器学习通用技术：sklearn快速入门</div></div></a></div><div><a href="/2024-8-3-LinearRegression.html" title="线性回归及其代码实现"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/玉子市场.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-03</div><div class="title">线性回归及其代码实现</div></div></a></div><div><a href="/2025-4-11-MatrixAlgebra.html" title="机器学习数学基础：矩阵微分"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景2.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-11</div><div class="title">机器学习数学基础：矩阵微分</div></div></a></div><div><a href="/2024-9-19-gradient_descent.html" title="深度学习基础：梯度下降"><img class="cover" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/声之形.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-19</div><div class="title">深度学习基础：梯度下降</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text"> 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text"> Pytorch构建模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#nnsequential"><span class="toc-number">2.1.</span> <span class="toc-text"> nn.Sequential</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model%E7%B1%BB"><span class="toc-number">2.2.</span> <span class="toc-text"> model类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text"> Pytorch训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvisiondatasets"><span class="toc-number">3.1.</span> <span class="toc-text"> torchvision.datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text"> 损失函数与优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E4%B8%8Egpu%E5%8A%A0%E9%80%9F"><span class="toc-number">3.3.</span> <span class="toc-text"> 训练流程与GPU加速</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95"><span class="toc-number">3.4.</span> <span class="toc-text"> 模型测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">3.5.</span> <span class="toc-text"> 模型保存与加载</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.</span> <span class="toc-text"> Pytorch读取数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#dataset%E7%B1%BB"><span class="toc-number">4.1.</span> <span class="toc-text"> Dataset类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="toc-number">4.2.</span> <span class="toc-text"> 自定义迭代器</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By HanCanon</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.19/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="//at.alicdn.com/t/c/font_4610099_r8r52ok711k.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024-3-11-HypothesisTesting-1.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景6.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-11</span><a class="blog-slider__title" href="2024-3-11-HypothesisTesting-1.html" alt="">假设检验（一）：参数检验</a><div class="blog-slider__text">介绍假设检验的思想与常见的检验方法</div><a class="blog-slider__button" href="2024-3-11-HypothesisTesting-1.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-15-DecisionTree-1.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/冰菓社刊.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-15</span><a class="blog-slider__title" href="2023-12-15-DecisionTree-1.html" alt="">决策树（上）：ID3算法与C4.5算法</a><div class="blog-slider__text">介绍决策树中的ID3算法与C4.5算法</div><a class="blog-slider__button" href="2023-12-15-DecisionTree-1.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-19-DecisionTree-2.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/冰菓社刊.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="2023-12-19-DecisionTree-2.html" alt="">决策树（下）：CART算法及其代码实现</a><div class="blog-slider__text">介绍主流的CART决策树算法及其sklearn代码实现</div><a class="blog-slider__button" href="2023-12-19-DecisionTree-2.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024-2-14-Generalization.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/虹夏头像.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-02-14</span><a class="blog-slider__title" href="2024-2-14-Generalization.html" alt="">机器学习理论：泛化误差、方差与偏差</a><div class="blog-slider__text">给出方差-偏差分解公式的详细推导并浅谈对实际建模的指导作用</div><a class="blog-slider__button" href="2024-2-14-Generalization.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-3-3-RandomForest.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/人2.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-03</span><a class="blog-slider__title" href="2023-3-3-RandomForest.html" alt="">随机森林原理与代码实现</a><div class="blog-slider__text">介绍随机森林算法的原理及其sklearn代码实现</div><a class="blog-slider__button" href="2023-3-3-RandomForest.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024-3-23-GBDT.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景1.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-23</span><a class="blog-slider__title" href="2024-3-23-GBDT.html" alt="">GBDT算法原理及其代码实现</a><div class="blog-slider__text">介绍GBDT算法的原理及其sklearn代码实现</div><a class="blog-slider__button" href="2024-3-23-GBDT.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024-3-16-ABTest.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景7.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-16</span><a class="blog-slider__title" href="2024-3-16-ABTest.html" alt="">A/B测试简介</a><div class="blog-slider__text">介绍A/B测试的流程与理论基础</div><a class="blog-slider__button" href="2024-3-16-ABTest.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024-3-15-MinimalSample.html" alt=""><img width="48" height="48" src="https://han-canon-picture.oss-accelerate.aliyuncs.com/风景7.webp" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-03-15</span><a class="blog-slider__title" href="2024-3-15-MinimalSample.html" alt="">A/B测试中最小样本量计算公式的推导</a><div class="blog-slider__text">介绍A/B测试中最小样本量公式的推导</div><a class="blog-slider__button" href="2024-3-15-MinimalSample.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>